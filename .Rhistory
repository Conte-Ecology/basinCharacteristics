names(conversionFactors) <- c("variable", "factor")
# ========================
# Read user-defined inputs
# ========================
source( file.path(baseDirectory, "scripts", "INPUTS.txt") )
# Set the directory where the tables are located
rTablesDirectory <- file.path(baseDirectory, "versions", outputName, "rTables")
if(!exists("outputVariables")){outputVariables <- c(discreteRasters, continuousRasters)}
outputVariables
# Create list of variables to compile
if ( all(outputVariables %in% "ALL" == TRUE) ){
localStatFiles <- list.files(path = rTablesDirectory, pattern = "local_")
}else{
localStatFiles <- c()
for( LF in seq_along(outputVariables) ){
localStatFiles <- c(localStatFiles, list.files(path = rTablesDirectory, pattern = paste0("local_",outputVariables[LF] ) ) )
}
}
localStatFiles
# Loop through files. Pull data and join together for output.
for ( L in seq_along(localStatFiles) ){
localTemp <- read.csv(file.path(rTablesDirectory, localStatFiles[L]) )
# Get file name
A <- gsub("*local_", "", localStatFiles[L])
variableName <- gsub(paste0("*_", statType,".csv"), "", A)
variableName <- gsub(paste0("*.csv"), "", variableName)
# Rename the columns. Account for the variables without the "percentAreaWithData" metric
if(ncol(localTemp) == 3) {names(localTemp) <- c(zoneField, variableName, paste0(variableName, "_percentAreaWithData"))}
else(names(localTemp) <- c(zoneField, variableName))
# Pull the variable specifc factor
factor <- filter(conversionFactors, variable == variableName)%>%
select(factor)
# Multiply the raw variable value by the conversion factor
localTemp[,names(localTemp) == variableName] <- localTemp[,names(localTemp) == variableName]*as.numeric(factor)
# Join to main dataframe
if( L == 1) {LocalStats <- localTemp} else(LocalStats <- left_join(LocalStats, localTemp, by = zoneField) )
}
head(LocalStats)
length(which(is.na(LocalStats$deg_barr_1)))
localStatFiles
L = 10
print(L)
localTemp <- read.csv(file.path(rTablesDirectory, localStatFiles[L]) )
localTemp
head(localTemp)
# Get file name
A <- gsub("*local_", "", localStatFiles[L])
A
variableName <- gsub(paste0("*_", statType,".csv"), "", A)
variableName
variableName <- gsub(paste0("*.csv"), "", variableName)
variableName
head(localTemp)
if(ncol(localTemp) == 3) {names(localTemp) <- c(zoneField, variableName, paste0(variableName, "_percentAreaWithData"))}
else(names(localTemp) <- c(zoneField, variableName))
if(ncol(localTemp) == 3) {names(localTemp) <- c(zoneField, variableName, paste0(variableName, "_percentAreaWithData"))} else(names(localTemp) <- c(zoneField, variableName))
head(localTemp)
# Pull the variable specifc factor
factor <- filter(conversionFactors, variable == variableName)%>%
select(factor)
factor
dim(factor)
length(factor)
as.numeric(factor)
is.na(as.numeric(factor))
if(is.na(as.numeric(factor))) {print("Factor missing for: ", variableName, ". Assigning a factor of 1.")}
as.numeric(factor)
is.na(as.numeric(factor))
if(is.na(as.numeric(factor))) print(paste0("Factor missing for: ", variableName, ". Assigning a factor of 1."))
if(is.na(as.numeric(factor))) print(paste0("Factor missing for: '", variableName, "'. Assigning a factor of 1."))
if(is.na(as.numeric(factor))) print(paste0("Factor missing for '", variableName, "'. Assigning a factor of 1."))
head(LocalStats)
if(is.na(as.numeric(factor))) {
print(paste0("Factor missing for '", variableName, "'. Assigning a default factor of 1."))
factor <- 1
}
factor
# Loop through files. Pull data and join together for output.
for ( L in seq_along(localStatFiles) ){
# Print status
print(L)
localTemp <- read.csv(file.path(rTablesDirectory, localStatFiles[L]) )
# Get file name
A <- gsub("*local_", "", localStatFiles[L])
variableName <- gsub(paste0("*_", statType,".csv"), "", A)
variableName <- gsub(paste0("*.csv"), "", variableName)
# Rename the columns. Account for the variables without the "percentAreaWithData" metric
if(ncol(localTemp) == 3) {names(localTemp) <- c(zoneField, variableName, paste0(variableName, "_percentAreaWithData"))} else(names(localTemp) <- c(zoneField, variableName))
# Pull the variable specifc factor
factor <- filter(conversionFactors, variable == variableName)%>%
select(factor)
if(is.na(as.numeric(factor))) {
print(paste0("Factor missing for '", variableName, "'. Assigning a default factor of 1."))
factor <- 1
}
# Multiply the raw variable value by the conversion factor
localTemp[,names(localTemp) == variableName] <- localTemp[,names(localTemp) == variableName]*as.numeric(factor)
# Join to main dataframe
if( L == 1) {LocalStats <- localTemp} else(LocalStats <- left_join(LocalStats, localTemp, by = zoneField) )
}
rm(list=ls())
# ===========
# Description
# ===========
# This script reads the output from the ArcPy zonal statistics script. It uses the table with polygon areas to calculate the upstream average of the variable.
# The averaging does not account for the overlapping areas between the polygons. Currently this is an unaddressed error.
# ==============
# Load libraries
# ==============
library(reshape2)
library(foreign)
library(tcltk)
library(dplyr)
library(lazyeval)
#=======================
# Set the Base Directory
#=======================
# Set the Base Directory
baseDirectory <- 'C:/KPONEIL/GitHub/projects/basinCharacteristics/zonalStatistics'
# ==========
# Load files
# ==========
# User inputs
# -----------
source( file.path(baseDirectory, "scripts", "RB_INPUTS.txt") )
bufferID
ls()
catchmentsFilePath
outputName
# Delineated catchments
# ---------------------
load(file.path(baseDirectory, "versions/NortheastHRD/NortheastHRD_delineatedCatchments.RData"))
# Catchment Areas
# ---------------
vectorArea <- read.dbf(file.path(baseDirectory, 'versions', outputName,'gisTables',  paste0('AreaSqKM_', bufferID,'.dbf')))
# ========================
# Process local statistics
# ========================
# Save local area as CSV
areaFilePath <- file.path(baseDirectory, 'versions', outputName,'rTables',  paste0('local_AreaSqKM_', bufferID,'.csv'))
if(!file.exists(areaFilePath)){
write.csv(vectorArea, areaFilePath, row.names = F)
}
# Loop through layers, reading files.
for (j in 1:length(rasterList)){
# File path to table
tableFilePath <- file.path(baseDirectory,"versions", outputName, "gisTables", paste0(rasterList[j], "_", bufferID, ".dbf"))
# Open table
dbfTable <-read.dbf(tableFilePath)[,c(zoneField, statType, "AREA")]
dbfTable[which(dbfTable[,statType] == -9999), statType] <- NA # Replace all "-9999" values with "NA"
# Save as CSV
write.csv(dbfTable[,c(zoneField, statType)],
file = file.path(baseDirectory, "versions", outputName, "rTables", paste0("local_", rasterList[j], "_", bufferID, ".csv")),
row.names = F)
# Prep dataframes for upstream averaging
# --------------------------------------
# Data
dat <- dbfTable[,c(zoneField, statType)]
names(dat)[2] <- rasterList[j]
if ( j == 1 ) { zonalData <- dat } else( zonalData <- left_join(zonalData, dat, by = zoneField))
# Areas
wt <- dbfTable[,c(zoneField, "AREA")]
names(wt)[2] <- rasterList[j]
if ( j == 1 ) { zonalAreas <- wt } else( zonalAreas <- left_join(zonalAreas, wt, by = zoneField))
}
# ===========================
# Process upstream statistics
# ===========================
# Define features to compute
featureList <- zonalData[,zoneField]
# Define storage dataframes
# -------------------------
# Upstream stats
upstreamStats <- data.frame(matrix(NA, nrow = length(featureList), ncol = length(rasterList) + 1))
names(upstreamStats) <- c(zoneField, rasterList)
## Areas with data
#pcntUpstreamWithData <- data.frame(matrix(NA, nrow = length(featureList), ncol = length(rasterList) + 1))
#names(pcntUpstreamWithData) <- c(zoneField, rasterList)
# Upstream area (from vector)
areaFileUpstream <- file.path(baseDirectory, "versions", outputName, "rTables", paste0("upstream_AreaSqKM_", bufferID, ".csv"))
# If Upstream area file doesn't exist, calculate it based on the vectors
if ( !file.exists(areaFileUpstream) ){
upstreamArea <- data.frame(matrix(NA, nrow = length(featureList), ncol = 2))
names(upstreamArea) <- c(zoneField, "AreaSqKM")
}
# Catchments loop
# ---------------
progressBar <- tkProgressBar(title = "progress bar", min = 0, max = length(featureList), width = 300)
for ( m in seq_along(featureList)){
# Get features in current basin
features <- delineatedCatchments[[which(names(delineatedCatchments) == featureList[m])]]
## Sum the areas of the individual catchments in the basin (raster version)
#TotDASqKM <- sum(filter_(rasterArea, interp(~col %in% features, col = as.name(zoneField)))$AreaSqKM)
# Get individual catchment stats for current basin
catchStats <- filter_(zonalData, interp(~col %in% features, col = as.name(zoneField)))#%>%
# Get individual catchment areas with data for current basin
catchAreas <- filter_(zonalAreas, interp(~col %in% features, col = as.name(zoneField)))#%>%
# Calculate the weights of each element in the dataframe (creates a matching dataframe)
weights <- sweep(catchAreas, 2, colSums(catchAreas), `/`)
# Sum the weighted stats to get final values
outStats <- colSums(catchStats*weights, na.rm = T)
# Get the percentage of catchment area with data
#outAreas <- colSums(catchAreas)/TotDASqKM
## Account for the rare case of catchments area = 0 (product of rasterizing catchments polygons)
#if (TotDASqKM == 0) {
#  outStats[2:length(outStats)] <- NA
#  outAreas[2:length(outAreas)] <- 0
#}
# Upstream stats
upstreamStats[m,1]                     <- featureList[m]
upstreamStats[m,2:ncol(upstreamStats)] <- outStats[-1]
## Area with data
#pcntUpstreamWithData[m,1]                     <- featureList[m]
#pcntUpstreamWithData[m,2:ncol(upstreamStats)] <- outAreas[-1]
# Total drainage area
# -------------------
# This is calculated based on the vector file
if ( !file.exists(areaFileUpstream) ){
upstreamArea[m,1] <- featureList[m]
upstreamArea[m,2] <- sum(filter_(vectorArea, interp(~col %in% features, col = as.name(zoneField)))$AreaSqKM, na.rm = T)
}
# Progress bar update
setTkProgressBar(progressBar, m, label=paste( round(m/length(featureList)*100, 2), "% done"))
}
rm(list=ls())
# ==============
# Load libraries
# ==============
library(reshape2)
library(foreign)
library(tcltk)
library(dplyr)
library(lazyeval)
#=======================
# Set the Base Directory
#=======================
baseDirectory <- 'C:/KPONEIL/GitHub/projects/basinCharacteristics/zonalStatistics'
# ==========
# Load files
# ==========
# User inputs
# -----------
source( file.path(baseDirectory, "scripts", "HRD_INPUTS.txt") )
rasterList <- c(discreteRasters, continuousRasters)
# If one of the raster lists is empty, remove the NA
rasterList <- rasterList[ -which(rasterList == "NA")]
rasterList
discreteRasters
continuousRasters
which(rasterList == "NA")
rasterList[ -which(rasterList == "NA")]
"NA" in rasterList
rasterList
rasterList <- c(discreteRasters, continuousRasters)
rasterList
"NA" in rasterList
"NA" %in% rasterList
rasterList <- c(discreteRasters, continuousRasters)
# If one of the raster lists is empty, remove the NA
if("NA" %in% rasterList){
rasterList <- rasterList[ -which(rasterList == "NA")]
}
rasterList
# Delineated catchments
# ---------------------
load(file.path(baseDirectory, "versions", outputName, paste0(outputName, "_delineatedCatchments.RData")))
# ==============
# Drainage Areas
# ==============
# The areas based on the vectors are saved as the areas, though the raster areas are used to define the percentage of area with data.
#   These areas match up with the data layers which
# Vector
# ------
# Input file
vectorFile <- file.path(baseDirectory, 'gisFiles/vectors', paste0(catchmentsFileName, '.dbf'))
# Output filepath (local)
areaFileLocal <- file.path(baseDirectory, "versions", outputName, "rTables", paste0("local_AreaSqKM.csv"))
# If the area file doesn't exist, write it. Else load it.
if ( !file.exists(areaFileLocal) ){
# Read the catchment attributes
vectorArea <- read.dbf(vectorFile)[,c(zoneField, "AreaSqKM")]
# Save file
write.csv(vectorArea, file = areaFileLocal, row.names = F)
} else {vectorArea <- read.csv(areaFileLocal)}
# Raster
# ------
rasterArea <- read.dbf(file.path(baseDirectory,"versions", outputName, "gisTables/catRasterAreas.dbf"))[,c(zoneField, "AREASQKM")]
names(rasterArea)[2] <- "AreaSqKM"
# ========================
# Process local statistics
# ========================
# Loop through layers, reading files.
for (j in 1:length(rasterList)){
# File path to table
tableFilePath <- file.path(baseDirectory,"versions", outputName, "gisTables", paste0(rasterList[j], "_", statType, ".dbf"))
# Open table
dbfTable <-read.dbf(tableFilePath)[,c(zoneField, statType, "AREA")]
dbfTable$AREA <- dbfTable$AREA*0.000001 # convert to square kilometers
dbfTable[which(dbfTable[,statType] == -9999), statType] <- NA # Replace all "-9999" values with "NA"
# Output filepath
outputTable <- file.path(baseDirectory, "versions", outputName, "rTables", paste0("local_", rasterList[j], "_", statType, ".csv"))
if ( !file.exists(outputTable) ){
# Calculate the % of the catchment area with data and include in the output
gisStat <- left_join(dbfTable, rasterArea, by = zoneField) %>%
mutate(percentAreaWithData = AREA/AreaSqKM*100)%>%
select(-c(AREA, AreaSqKM))
# save this as a file
write.csv(gisStat, file = outputTable, row.names = F)
}
# Prep dataframes for upstream averaging
# --------------------------------------
# Data
dat <- dbfTable[,c(zoneField, statType)]
names(dat)[2] <- rasterList[j]
if ( j == 1 ) { zonalData <- dat } else( zonalData <- left_join(zonalData, dat, by = zoneField))
# Areas
wt <- dbfTable[,c(zoneField, "AREA")]
names(wt)[2] <- rasterList[j]
if ( j == 1 ) { zonalAreas <- wt } else( zonalAreas <- left_join(zonalAreas, wt, by = zoneField))
}
# ===========================
# Process upstream statistics
# ===========================
# Define features to compute
featureList <- zonalData[,zoneField]
# Define storage dataframes
# -------------------------
# Upstream stats
upstreamStats <- data.frame(matrix(NA, nrow = length(featureList), ncol = length(rasterList) + 1))
names(upstreamStats) <- c(zoneField, rasterList)
# Areas with data
pcntUpstreamWithData <- data.frame(matrix(NA, nrow = length(featureList), ncol = length(rasterList) + 1))
names(pcntUpstreamWithData) <- c(zoneField, rasterList)
# Upstream area (from vector)
areaFileUpstream <- file.path(baseDirectory, "versions", outputName, "rTables", paste0("upstream_AreaSqKM.csv"))
# If Upstream area file doesn't exist, calculate it based on the vectors
if ( !file.exists(areaFileUpstream) ){
upstreamArea <- data.frame(matrix(NA, nrow = length(featureList), ncol = 2))
names(upstreamArea) <- c(zoneField, "AreaSqKM")
}
# Catchments loop
# ---------------
progressBar <- tkProgressBar(title = "progress bar", min = 0, max = length(featureList), width = 300)
for ( m in seq_along(featureList)){
# Get features in current basin
features <- delineatedCatchments[[which(names(delineatedCatchments) == featureList[m])]]
# Sum the areas of the individual catchments in the basin (raster version)
TotDASqKM <- sum(filter_(rasterArea, interp(~col %in% features, col = as.name(zoneField)))$AreaSqKM)
# Get individual catchment stats for current basin
catchStats <- filter_(zonalData, interp(~col %in% features, col = as.name(zoneField)))#%>%
# Get individual catchment areas with data for current basin
catchAreas <- filter_(zonalAreas, interp(~col %in% features, col = as.name(zoneField)))#%>%
# Calculate the weights of each element in the dataframe (creates a matching dataframe)
weights <- sweep(catchAreas, 2, colSums(catchAreas), `/`)
# Weight the values by area
weightedStats <- catchStats*weights
# Sum the weighted stats to get final values. (Account for the case where all values are NA, preventing it from returning a 0)
outStats <- colSums(weightedStats, na.rm=TRUE) + ifelse(colSums(is.na(weightedStats)) == nrow(weightedStats), NA, 0)
# Get the percentage of catchment area with data
outAreas <- colSums(catchAreas)/TotDASqKM
# Account for the rare case of catchments area = 0 (product of rasterizing catchments polygons)
if (TotDASqKM == 0) {
outStats[2:length(outStats)] <- NA
outAreas[2:length(outAreas)] <- 0
}
# Upstream stats
upstreamStats[m,1]                     <- featureList[m]
upstreamStats[m,2:ncol(upstreamStats)] <- outStats[-1]
# Area with data
pcntUpstreamWithData[m,1]                     <- featureList[m]
pcntUpstreamWithData[m,2:ncol(upstreamStats)] <- outAreas[-1]
# Total drainage area
# -------------------
# This is calculated based on the vector file
if ( !file.exists(areaFileUpstream) ){
upstreamArea[m,1] <- featureList[m]
upstreamArea[m,2] <- sum(filter_(vectorArea, interp(~col %in% features, col = as.name(zoneField)))$AreaSqKM, na.rm = T)
}
# Progress bar update
setTkProgressBar(progressBar, m, label=paste( round(m/length(featureList)*100, 2), "% done"))
}
close(progressBar)
# Output upstream statistics tables
# ---------------------------------
# Loop through variables writing tables with upstream data and the percent of the area with data
for ( n in 2:(ncol(upstreamStats))){
# Name
colName <- names(upstreamStats)[n]
# Output dataframe
upStat <- upstreamStats[,c(zoneField, colName)]
names(upStat)[2] <- statType
upPcnt <- pcntUpstreamWithData[,c(zoneField, colName)]
upPcnt[,2] <- upPcnt[,2]*100
names(upPcnt)[2] <- "percentAreaWithData"
up <- left_join(upStat, upPcnt, by = zoneField)
outputUpstream  <- file.path(baseDirectory, "versions", outputName, "rTables", paste0("upstream_", colName, "_", statType, ".csv"))
write.csv(up, file = outputUpstream,  row.names = F)
}
# Save area file
if ( !file.exists(areaFileUpstream) ){
write.csv(upstreamArea, file = areaFileUpstream, row.names = F)
}
rm(list=ls())
# Catchment Stats Generator
library(dplyr)
library(reshape2)
# ======
# Inputs
# ======
baseDirectory <- 'C:/KPONEIL/GitHub/projects/basinCharacteristics/zonalStatistics'
# There are 3 options for specifying the variables to output:
#   1) "ALL" will include all of the variables present in the folder
#   2) NULL will include the variables from the "rasterList" object in the "HRD_INPUTS.txt" file
#   3) Manually list the variables to output
outputVariables <- c("ALL")
# ========================
# Read user-defined inputs
# ========================
source( file.path(baseDirectory, "scripts", "HRD_INPUTS.txt") )
# ==================
# Conversion Factors
# ==================
# Read the conversion factors file
setwd(baseDirectory); setwd('..')
conversionFactors <- read.csv("Covariate Data Status - High Res Delineation.csv")[,c("Name", "Conversion.Factor")]
# Rename columns
names(conversionFactors) <- c("variable", "factor")
# ======================
# Group stats for output
# ======================
# Set the directory where the tables are located
rTablesDirectory <- file.path(baseDirectory, "versions", outputName, "rTables")
if(is.null(outputVariables)){outputVariables <- c(discreteRasters, continuousRasters)}
# Local
# -----
# Create list of variables to compile
if ( all(outputVariables %in% "ALL" == TRUE) ){
localStatFiles <- list.files(path = rTablesDirectory, pattern = "local_")
}else{
localStatFiles <- c()
for( LF in seq_along(outputVariables) ){
localStatFiles <- c(localStatFiles, list.files(path = rTablesDirectory, pattern = paste0("local_",outputVariables[LF] ) ) )
}
}
# Loop through files. Pull data and join together for output.
for ( L in seq_along(localStatFiles) ){
# Print status
print(L)
localTemp <- read.csv(file.path(rTablesDirectory, localStatFiles[L]) )
# Get file name
A <- gsub("*local_", "", localStatFiles[L])
variableName <- gsub(paste0("*_", statType,".csv"), "", A)
variableName <- gsub(paste0("*.csv"), "", variableName)
# Rename the columns. Account for the variables without the "percentAreaWithData" metric
if(ncol(localTemp) == 3) {names(localTemp) <- c(zoneField, variableName, paste0(variableName, "_percentAreaWithData"))} else(names(localTemp) <- c(zoneField, variableName))
# Pull the variable specifc factor
factor <- filter(conversionFactors, variable == variableName)%>%
select(factor)
# Account for missing factors
if(is.na(as.numeric(factor))) {
print(paste0("Factor missing for '", variableName, "'. Assigning a default factor of 1."))
factor <- 1
}
# Multiply the raw variable value by the conversion factor
localTemp[,names(localTemp) == variableName] <- localTemp[,names(localTemp) == variableName]*as.numeric(factor)
# Join to main dataframe
if( L == 1) {LocalStats <- localTemp} else(LocalStats <- left_join(LocalStats, localTemp, by = zoneField) )
}
# Upstream
# --------
# Create list of variables to compile
if ( all(outputVariables %in% "ALL" == TRUE) ){
upstreamStatFiles <- list.files(path = rTablesDirectory, pattern = "upstream_")
}else{
upstreamStatFiles <- c()
for( UF in seq_along(outputVariables) ){
upstreamStatFiles <- c(upstreamStatFiles, list.files(path = rTablesDirectory, pattern = paste0("upstream_",outputVariables[UF] ) ) )
}
}
# Loop through files. Pull data and join together for output.
for ( U in 1:length(upstreamStatFiles) ){
upstreamTemp <- read.csv(file.path(rTablesDirectory, upstreamStatFiles[U]) )
# Get file name
A <- gsub("*upstream_", "", upstreamStatFiles[U])
variableName <- gsub(paste0("*_", statType,".csv"), "", A)
variableName <- gsub(paste0("*.csv"), "", variableName)
# Rename the columns. Account for the variables without the "percentAreaWithData" metric
if(ncol(upstreamTemp) == 3) {names(upstreamTemp) <- c(zoneField, variableName, paste0(variableName, "_percentAreaWithData"))}
else(names(upstreamTemp) <- c(zoneField, variableName))
# Pull the variable specific factor
factor <- filter(conversionFactors, variable == variableName)%>%
select(factor)
# Account for missing factors
if(is.na(as.numeric(factor))) {
print(paste0("Factor missing for '", variableName, "'. Assigning a default factor of 1."))
factor <- 1
}
# Multiply the raw variable value by the conversion factor
upstreamTemp[,names(upstreamTemp) == variableName] <- upstreamTemp[,names(upstreamTemp) == variableName]*as.numeric(factor)
# Join to main dataframe
if( U == 1) {UpstreamStats <- upstreamTemp} else(UpstreamStats <- left_join(UpstreamStats, upstreamTemp, by = zoneField) )
}
# Save output
save(LocalStats, UpstreamStats, file = file.path(baseDirectory, "versions", outputName, "completedStats", paste0("zonalStats", Sys.Date(),".RData") ))
# Format for Database
# ===================
locLong <- melt(LocalStats,'FEATUREID')
locLong$zone <- "local"
upLong <- melt(UpstreamStats,'FEATUREID')
upLong$zone <- "upstream"
dbStats <- rbind(locLong, upLong)
save(dbStats, file = file.path(baseDirectory, "versions", outputName, "completedStats", paste0("zonalStatsForDB_", Sys.Date(),".RData") ))
